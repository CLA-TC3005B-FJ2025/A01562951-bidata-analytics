### Preguntas de reflexión

1. Nos ayuda porque distribuye las operaciones entre varias máquinas lo que hace que el trabajo se ejecute en múltiples nódos. También hace que se reduzca la necesidad de leer y escribir constantemente en el disco.

2. Pyspark permite que se manejen mayor cantidad de terabytes de datos distribuyendo la carga de trabajo. También como trabaja en paralelo hace que supere a herramientas alternativas. 

3. Podríamos hacer un exploratory data analysis para poder conocer un poco nuestra muestar de datos. Hacer gráficas, box plots, entre otras cosas. También podemos hacer feature engineering, que es agregar nuevas features utilizando features ya existentes en nuestro dataframe. 